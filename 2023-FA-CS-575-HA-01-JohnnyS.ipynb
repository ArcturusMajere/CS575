{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2706e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings;warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd3206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parquet is an open source file format built to handle flat columnar storage data formats. \n",
    "\n",
    "Parquet works great with large, complex data and is known for its data compression and ability many encoding types.\n",
    "\n",
    "Data found: https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states/data\n",
    "\n",
    "size: 986.46 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parquet_path = \"train_series.parquet\"\n",
    "test_parquet_path = \"test_series.parquet\"\n",
    "train_events_path = \"train_events.csv\"\n",
    "train_series = pd.read_parquet(train_parquet_path)\n",
    "test_series = pd.read_parquet(test_parquet_path)\n",
    "train_events = pd.read_csv(train_events_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38511466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing known NANs,known incomplete series:31011ade7c0a,a596ad0b82aa\n",
    "series_NaN = train_events.groupby('series_id')['step'].apply(lambda x: x.isnull().any())\n",
    "non_NaN_series = series_NaN[~series_NaN].index.tolist()\n",
    "non_NaN_series.remove('31011ade7c0a')\n",
    "non_NaN_series.remove('a596ad0b82aa') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquet_genny(series):\n",
    "    train_series = pd.read_parquet(\"train_series.parquet\", filters=[('series_id','=',series)])\n",
    "    train_events = pd.read_csv(\"train_events.csv\").query('series_id == @series')\n",
    "    train_events = train_events.dropna()\n",
    "    train_events[\"step\"]  = train_events[\"step\"].astype(\"int\")\n",
    "    train_events[\"awake\"] = train_events[\"event\"].replace({\"onset\":1,\"wakeup\":0})\n",
    "    train = pd.merge(train_series, train_events[['step','awake']], on='step', how='left')\n",
    "    train[\"awake\"] = train[\"awake\"].bfill(axis ='rows')\n",
    "    train['awake'] = train['awake'].fillna(1)  \n",
    "    train[\"awake\"] = train[\"awake\"].astype(\"int\")\n",
    "    return(train)\n",
    "parquet_data = []\n",
    "for series_id in non_NaN_series:\n",
    "    train = parquet_genny(series_id)\n",
    "    parquet_data.append(train)\n",
    "    del train\n",
    "    gc.collect(); #memory help !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f70375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zzzs_train = pd.concat(parquet_data).reset_index(drop=True)\n",
    "print(Zzzs_train[\"series_id\"].nunique(), \"indivudual sleep training series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac13d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reset -f\n",
    "#file_path = 'clean_zzz.csv'\n",
    "#Zzzs_train.to_csv(file_path, index=False)\n",
    "#zzz = pd.read_csv(file_path)\n",
    "#clear mem - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda1876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating possible time covariates for model \n",
    "start_time = datetime.strptime(zzz['timestamp'].iloc[0], '%Y-%m-%dT%H:%M:%S%z')\n",
    "zzz['seconds_since_start'] = zzz['timestamp'].apply(lambda x: (datetime.strptime(x, '%Y-%m-%dT%H:%M:%S%z') - start_time).seconds)\n",
    "zzz = zzz.drop(columns=['timestamp', 'step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df3f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorte by time series_id & seconds_since_start\n",
    "zzz_timesort = zzz.sort_values(by=['series_id', 'seconds_since_start'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38c85b2",
   "metadata": {},
   "source": [
    "Z-angle: corresponds to the angle between the accelerometer axis perpendicular to the skin surface and the horizontal plane.\n",
    "\n",
    "ENMO : The Euclidean Norm Minus One (ENMO) with negative values rounded to zero in g has been shown to correlate with the magnitude of acceleration and human energy expenditure16. ENMO is computed as follows:\n",
    "\n",
    "$ \\text{ENMO} = \\sqrt{x^2 + y^2 + z^2} - 1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize feats: time(seconds since session start),anglez & enmo(Euclidean Norm Minus One)\n",
    "zzz_timesort['anglez_std'] = (zzz_timesort['anglez'] - zzz_timesort['anglez'].mean()) / zzz_timesort['anglez'].std()\n",
    "zzz_timesort['enmo_std'] = (zzz_timesort['enmo'] - zzz_timesort['enmo'].mean()) / zzz_timesort['enmo'].std()\n",
    "zzz_timesort['sss_std'] = (zzz_timesort['seconds_since_start'] - zzz_timesort['seconds_since_start'].mean()) / zzz_timesort['seconds_since_start'].std()\n",
    "\n",
    "zzz_timesort['anglez_raw'] = (zzz_timesort['anglez'])\n",
    "zzz_timesort['enmo_raw'] = (zzz_timesort['enmo'])\n",
    "zzz_timesort['sss_raw'] = (zzz_timesort['seconds_since_start'])\n",
    "\n",
    "\n",
    "feats = [col for col in zzz_timesort.columns if 'std' in col]\n",
    "X = zzz_timesort[feats]\n",
    "y = zzz_timesort['awake']\n",
    "\n",
    "feats_raw = [col for col in zzz_timesort.columns if 'raw' in col]\n",
    "X_raw = zzz_timesort[feats_raw]\n",
    "\n",
    "# split the standardized data (79-21)\n",
    "train_size = int(0.79 * len(zzz_timesort))\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# split the raw training & test data (79-21)\n",
    "X_tr, X_te = X_raw.iloc[:train_size], X_raw.iloc[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa92c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path1 = 'X1.csv'\n",
    "#file_path2 = 'y1.csv'\n",
    "#file_path3 = 'X2.csv'\n",
    "#file_path4 = 'y2.csv'\n",
    "#X_train.to_csv(file_path1, index=False)\n",
    "#y_train.to_csv(file_path2, index=False)\n",
    "#X_test.to_csv(file_path3, index=False)\n",
    "#y_test.to_csv(file_path4, index=False)\n",
    "\n",
    "#file_path5 = 'X1r.csv'\n",
    "#file_path6 = 'X2r.csv'\n",
    "#X_tr.to_csv(file_path5, index=False)\n",
    "#X_te.to_csv(file_path6, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear mem - \n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be98e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings;warnings.simplefilter(action='ignore', category=Warning)\n",
    "file_path1 = 'X1.csv'\n",
    "file_path2 = 'y1.csv'\n",
    "file_path3 = 'X2.csv'\n",
    "file_path4 = 'y2.csv'\n",
    "X1 = pd.read_csv(file_path1)\n",
    "y1 = pd.read_csv(file_path2)\n",
    "X2 = pd.read_csv(file_path3)\n",
    "y2 = pd.read_csv(file_path4)\n",
    "\n",
    "X_train = np.array(X1);X_test = np.array(X2)\n",
    "y_train = np.array(y1).flatten();y_test = np.array(y2).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8738bcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------+------------------+--------------+---------+-----------+------------+\n",
      "| Data       | Type                    | Shape            | MissingVals  | Size(MB)| Mean      | SD         |\n",
      "+------------+-------------------------+------------------+--------------+---------+-----------+------------+\n",
      "| X_train   | <class 'numpy.ndarray'>  | (10400792, 3)    | 0            | 238.06  | -0.00     | 0.99       |\n",
      "| y_train   | <class 'numpy.ndarray'>  | (10400792,)      | 0            | 79.35   | 0.66      | 0.47       |\n",
      "| X_test    | <class 'numpy.ndarray'>  | (2764768, 3)     | 0            | 63.28   | 0.01      | 1.04       |\n",
      "| y_test    | <class 'numpy.ndarray'>  | (2764768,)       | 0            | 21.09   | 0.64      | 0.48       |\n",
      "+------------+-------------------------+------------------+--------------+---------+-----------+------------+\n"
     ]
    }
   ],
   "source": [
    "# numpy only data peek\n",
    "\n",
    "def data_info(data):\n",
    "    missing_values = np.isnan(data).sum()\n",
    "    size_in_mb = data.nbytes / (1024 ** 2)\n",
    "    if len(data.shape) > 1:\n",
    "        mean = data.mean()\n",
    "        sd = data.std()\n",
    "    else:\n",
    "        mean = np.mean(data)\n",
    "        sd = np.std(data)\n",
    "    return missing_values, size_in_mb, mean, sd\n",
    "\n",
    "def data_summary(X_train, y_train, X_test, y_test):\n",
    "    header = \"+------------+-------------------------+------------------+--------------+---------+-----------+------------+\"\n",
    "    print(header)\n",
    "    print(\"| Data       | Type                    | Shape            | MissingVals  | Size(MB)| Mean      | SD         |\")\n",
    "    print(header)\n",
    "\n",
    "    for name, data in [(\"X_train\", X_train), (\"y_train\", y_train), (\"X_test\", X_test), (\"y_test\", y_test)]:\n",
    "        missing_values, size_in_mb, mean, sd = data_info(data)\n",
    "        print(\"| {:<9} | {:<24} | {:<16} | {:<12} | {:<7.2f} | {:<9.2f} | {:<10.2f} |\".format(\n",
    "            name, str(type(data)), str(data.shape), missing_values, size_in_mb, mean, sd))\n",
    "    print(header)\n",
    "\n",
    "\n",
    "data_summary(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada67047",
   "metadata": {},
   "source": [
    "Covariates standardized:\n",
    "\n",
    "column 1: Z-angle, corresponds to the angle between the accelerometer axis perpendicular to the skin surface and the horizontal plane.\n",
    "\n",
    "${angl}{{e}}_{{z}}=({ta}{{n}}^{-1}\\frac{{{a}}_{{z}}}{{{a}}_{x}^{2}+{{a}}_{y}^{2}})\\cdot 180/\\pi ,$\n",
    "\n",
    "column 2: ENMO, The Euclidean Norm Minus One with negative values rounded to zero in g has been shown to correlate with the magnitude of acceleration and human energy expenditure16. ENMO is computed as follows:\n",
    "\n",
    "$ \\text{ENMO} = \\sqrt{x^2 + y^2 + z^2} - 1 $\n",
    "\n",
    "column3: sss, seconds since session start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c2e348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Stuff\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def accuracy(TP, TN, FP, FN):\n",
    "    return (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "def specificity(TN, FP):\n",
    "    return TN / (TN + FP)\n",
    "\n",
    "def sensitivity(TP, FN):\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def f1_score(TP, FP, FN):\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def print_confusion_matrix(TP, TN, FP, FN):\n",
    "    head_cm = \"+-----------------+------------+------+\"\n",
    "    print(head_cm)\n",
    "    print(\"|                 | Predicted        |\")\n",
    "    print(\"| Actual          |      0    |   1  |\")\n",
    "    print(head_cm)\n",
    "    print(\"| 0               | {:<5} | {:<5}   |\".format(TN, FP))\n",
    "    print(\"| 1               | {:<5} | {:<5}  |\".format(FN, TP))\n",
    "    print(head_cm)\n",
    "    print()\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    TP, TN, FP, FN = confusion_matrix(y_true, y_pred)\n",
    "    acc = accuracy(TP, TN, FP, FN)\n",
    "    spec = specificity(TN, FP)\n",
    "    sens = sensitivity(TP, FN)\n",
    "    f1 = f1_score(TP, FP, FN)\n",
    "\n",
    "    print_confusion_matrix(TP, TN, FP, FN)\n",
    "    head = \"+--------------+------------+\"\n",
    "    print(head)\n",
    "    print(\"| Metric       | Value      |\")\n",
    "    print(head)\n",
    "    print(\"| Accuracy     | {:10.4f} |\".format(acc))\n",
    "    print(\"| Specificity  | {:10.4f} |\".format(spec))\n",
    "    print(\"| Sensitivity  | {:10.4f} |\".format(sens))\n",
    "    print(\"| F1 Score     | {:10.4f} |\".format(f1))\n",
    "    print(head)\n",
    "\n",
    "def linear_mod(bias, theta):\n",
    "    terms = [\n",
    "    f\"{bias:.8f} * bias(intercept)\",\n",
    "    f\"{theta[0]:.8f} * angleZ\",\n",
    "    f\"{theta[1]:.8f} * EuclideanNormMinusOne\",\n",
    "    f\"{theta[2]:.8f} * SecondsSinceStart\" ]\n",
    "    return \" + \".join(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33524fba",
   "metadata": {},
   "source": [
    "Logistic regression models the probability that the target variable belongs to a particular category. \n",
    "\n",
    "It uses the logistic function to squeeze the output of a linear equation between 0 and 1.\n",
    "\n",
    "Given the weights\n",
    "$\\mathbf{w}$ and input $\\mathbf{x}$, the logistic function is:\n",
    "\n",
    "$P(y=1|\\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{w}^T \\mathbf{x}}}$\n",
    "\n",
    "To fit a logistic regression model while restricted to numpy, I used gradient descent.\n",
    "\n",
    "Keep in mind a logistic regression is truely a classification, so the metrics are adjusted to such.\n",
    "\n",
    "The gradient of the logistic loss with respect to the weights is:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\mathbf{w}} = (\\mathbf{y} - \\hat{\\mathbf{y}}) \\mathbf{x}$\n",
    "\n",
    "Where:\n",
    "$  L $ is the logistic loss.\n",
    "$\\mathbf{y}$ is the true label.\n",
    "$\\hat{\\mathbf{y}}$ is the predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a6338cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary target: logisitic regression which is acutally classificaiton.\n",
    "\n",
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, lr, iter):\n",
    "        self.lr = lr\n",
    "        self.iter = iter\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # initialization of weights\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "\n",
    "        for i in range(self.iter):\n",
    "            model = np.dot(X, self.theta) + self.bias\n",
    "            predictions = self.sigmoid(model)\n",
    "\n",
    "            # compute gradient\n",
    "            d_theta = (1 / len(X)) * np.dot(X.T, (predictions - y))\n",
    "            d_bias = (1 / len(X)) * np.sum(predictions - y)\n",
    "\n",
    "            # update my weights\n",
    "            self.theta -= self.lr * d_theta\n",
    "            self.bias -= self.lr * d_bias\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.theta) + self.bias)\n",
    "\n",
    "    def predict(self, X, threshold=0.500):\n",
    "        return (self.predict_prob(X) >= threshold).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "472d5af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.25\n",
      "+-----------------+------------+------+\n",
      "|                 | Predicted        |\n",
      "| Actual          |      0    |   1  |\n",
      "+-----------------+------------+------+\n",
      "| 0               | 811402 | 170937   |\n",
      "| 1               | 250730 | 1531699  |\n",
      "+-----------------+------------+------+\n",
      "\n",
      "+--------------+------------+\n",
      "| Metric       | Value      |\n",
      "+--------------+------------+\n",
      "| Accuracy     |     0.8475 |\n",
      "| Specificity  |     0.8260 |\n",
      "| Sensitivity  |     0.8593 |\n",
      "| F1 Score     |     0.8790 |\n",
      "+--------------+------------+\n",
      "None\n",
      "1.80955572 * bias(intercept) + 0.16049870 * angleZ + 3.90823003 * EuclideanNormMinusOne + -1.51501577 * SecondsSinceStart\n",
      "learning rate 0.1\n",
      "+-----------------+------------+------+\n",
      "|                 | Predicted        |\n",
      "| Actual          |      0    |   1  |\n",
      "+-----------------+------------+------+\n",
      "| 0               | 785927 | 196412   |\n",
      "| 1               | 259182 | 1523247  |\n",
      "+-----------------+------------+------+\n",
      "\n",
      "+--------------+------------+\n",
      "| Metric       | Value      |\n",
      "+--------------+------------+\n",
      "| Accuracy     |     0.8352 |\n",
      "| Specificity  |     0.8001 |\n",
      "| Sensitivity  |     0.8546 |\n",
      "| F1 Score     |     0.8699 |\n",
      "+--------------+------------+\n",
      "None\n",
      "1.41555474 * bias(intercept) + 0.12119274 * angleZ + 2.42522668 * EuclideanNormMinusOne + -1.48907399 * SecondsSinceStart\n",
      "learning rate 0.01\n",
      "+-----------------+------------+------+\n",
      "|                 | Predicted        |\n",
      "| Actual          |      0    |   1  |\n",
      "+-----------------+------------+------+\n",
      "| 0               | 752363 | 229976   |\n",
      "| 1               | 301596 | 1480833  |\n",
      "+-----------------+------------+------+\n",
      "\n",
      "+--------------+------------+\n",
      "| Metric       | Value      |\n",
      "+--------------+------------+\n",
      "| Accuracy     |     0.8077 |\n",
      "| Specificity  |     0.7659 |\n",
      "| Sensitivity  |     0.8308 |\n",
      "| F1 Score     |     0.8478 |\n",
      "+--------------+------------+\n",
      "None\n",
      "0.70510873 * bias(intercept) + 0.06283974 * angleZ + 0.51734549 * EuclideanNormMinusOne + -1.07741477 * SecondsSinceStart\n",
      "learning rate 0.001\n",
      "+-----------------+------------+------+\n",
      "|                 | Predicted        |\n",
      "| Actual          |      0    |   1  |\n",
      "+-----------------+------------+------+\n",
      "| 0               | 760762 | 221577   |\n",
      "| 1               | 309314 | 1473115  |\n",
      "+-----------------+------------+------+\n",
      "\n",
      "+--------------+------------+\n",
      "| Metric       | Value      |\n",
      "+--------------+------------+\n",
      "| Accuracy     |     0.8080 |\n",
      "| Specificity  |     0.7744 |\n",
      "| Sensitivity  |     0.8265 |\n",
      "| F1 Score     |     0.8473 |\n",
      "+--------------+------------+\n",
      "None\n",
      "0.14246171 * bias(intercept) + 0.01533829 * angleZ + 0.08987027 * EuclideanNormMinusOne + -0.23405911 * SecondsSinceStart\n",
      "learning rate 1e-06\n",
      "+-----------------+------------+------+\n",
      "|                 | Predicted        |\n",
      "| Actual          |      0    |   1  |\n",
      "+-----------------+------------+------+\n",
      "| 0               | 764637 | 217702   |\n",
      "| 1               | 309866 | 1472563  |\n",
      "+-----------------+------------+------+\n",
      "\n",
      "+--------------+------------+\n",
      "| Metric       | Value      |\n",
      "+--------------+------------+\n",
      "| Accuracy     |     0.8092 |\n",
      "| Specificity  |     0.7784 |\n",
      "| Sensitivity  |     0.8262 |\n",
      "| F1 Score     |     0.8481 |\n",
      "+--------------+------------+\n",
      "None\n",
      "0.00016116 * bias(intercept) + 0.00001810 * angleZ + 0.00010527 * EuclideanNormMinusOne + -0.00026629 * SecondsSinceStart\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.25, 0.1, 0.01, 0.001,0.000001]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model_logreg = LogisticRegression(lr=lr, iter=1000)\n",
    "    model_logreg.fit(X_train, y_train)\n",
    "    y_pred_logreg = model_logreg.predict(X_test)\n",
    "    print(\"learning rate\",lr)\n",
    "    print(metrics(y_test,y_pred_logreg))\n",
    "    model_str = linear_mod(model_logreg.bias, model_logreg.theta)\n",
    "    print(model_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ba02d",
   "metadata": {},
   "source": [
    "Even though I will not persue a linear model in this manner, I successfully reached 84% accuracy with a 0.25 learning rate.\n",
    "\n",
    "Linear model found: \n",
    "\n",
    "1.80955572 * bias(intercept) + 0.16049870 * angleZ + 3.90823003 * EuclideanNormMinusOne + -1.51501577 * SecondsSinceStart\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008de65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
